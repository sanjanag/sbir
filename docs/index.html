<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Computer Vision Class Project
        | CS, Georgia Tech | Fall 2019: CS 6476</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="css/bootstrap.css" rel="stylesheet">
    <style>
        body {
            padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
        }

        .vis {
            color: #3366CC;
        }

        .data {
            color: #FF9900;
        }
    </style>

    <link href="css/bootstrap-responsive.min.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
</head>

<body>
<div class="container">
    <div class="page-header">

        <!-- Title and Name -->
        <h1>Sketch-Based Image Retrieval</h1>
        <span style="font-size: 20px; line-height: 1.5em;"><strong>Ramya Sree Boppana (903456349), Ang Deng (902989694), and Sanjana Garg (903475801)</strong></span><br>
        <span style="font-size: 18px; line-height: 1.5em;">Fall 2019: CS 6476 Computer Vision: Class Project</span><br>Georgia
        Tech</span>
        <hr>

        <h4> <a href="proposal.html"> Proposal </a></h4>
        <!-- Introduction -->
        <h3 style="color: #3366CC">Abstract</h3>



        Sketch based image retrieval systems have many applications in daily life like
        Medical diagnosis, digital library, search engines, crime prevention,
        photo sharing sites, geographical information, and sensing remote
        systems. The traditional text-based image retrieval systems are limited
        as with growing database of images it is difficult and unrealistic to
        annotate all the images for text-based search.
        <br>
        In order to tackle this problem, we build a pipeline combining preprocessing the images (gray scaling edge extraction) and extracting various feature descriptors (SIFT, HOG, and Hu Moments)
        from the images. Then, we take the extracted features from a input sketch and fetch the most similar k images from the database based on distance metrics corresponding to each type of feature descriptors.
        <br>
        Based on the test dataset which contains images from two different categories split in 50/50 way,
        we were able to achieve above baseline (50%) accuracy from all three descriptors that we implemented. Our next step will be to try to tweak the parameters of these feature
        extractors and to try out strategies to combine the features in order to improve the
        overall performance.



        <h3 style="color: #3366CC">Teaser Figure</h3>
        Sample output from our application:<br>

        &emsp;&emsp;&emsp;&emsp;&emsp;
        <img style="height: 120px;" alt="" src="chair16.png">
        <img style="height: 300px;" alt="" src="hog_chair16.png">


        <br><br>

        <h3 style="color: #3366CC">Introduction</h3>

        The traditional text-based image retrieval systems are limited as with growing database of images it is difficult and unrealistic to annotate all the images for text-based search. Using sketch to retrieve images also alleviates the need for generating accurate captions for existing images, which becomes a complex natural language processing problem as the description is required to be more specific and detailed. Sketches have a much greater potential to describe the content and exact details of the image than plain text and are a more direct way of expressing human thoughts than text abstraction.<br>
        Sketch based image retrieval systems have many applications in daily life like Medical diagnosis, digital library, search engines, crime prevention, photo sharing sites, geographical information, and sensing remote systems. With the increasing use of touch screen devices, Sketch-Based Image Retrieval (SBIR) has relevant applications in e-commerce platforms also. <br>
        Currently we are with regular RBG photographs, but the skeches are only black and white.
        We are not designing any new way to solve this problem, but building from scratch using knowledge we obtained from class lectures and reseaches.<br>


        <!-- Approach -->
        <h3 style="color: #3366CC">Approach</h3>
        In our approach, we aim to tackle the following aspects of sketch-image
        differences:
        <ul>
            <li><b>Visual Cue Imbalance:</b> The sketches have a holistic shape
                and salient local shapes while the images are abundant in
                details on shape, texture, and color.
            </li>
            <li><b>Abstraction Gap:</b> The sketches are usually simplified
                version (missing details) of images with random distortion (the
                randomness in strokes) and unrealistic disproportion (object
                parts being unrealistically smaller or bigger).
            </li>
        </ul>

        <div style="text-align: center;">
            <img style="height: 250px;" alt=""
                 src="proposal_approach_flow_diagram.png">
        </div>
        The whole SBIR framework can be divided into two phases- pre-processing
        and retrieval. The framework is shown in the above figure.
        <h4>Pre-Processing Phase</h4>
        In this phase, we pre-process the dataset to extract features. For
        every image in the dataset, we perform the following steps.
        <h5>Feature/Edge extraction</h5>
        Convert the image to its gray intensity representation. Extract local edge or global shape features
        and encode them in a feature vector using representations like histogram.
        The histogram based features are efficient for similarity comparison
        and they can also tolerate random distortions in the sketches
        due to the grid division scheme in feature extraction.
        Edge/shape extraction addresses the visual cue imbalance. Since the sketches
        are generally composed of strokes which are mostly edges, it is
        intuitive to compare edge maps of images with sketches. This also addresses
        the simplification sub-aspect of the abstraction gap.
        <br>
        Some of the feature extraction methods we tried are described below. <br>
        <ol>
            <b> <li>  HOG - Histogram of Oriented Gradients  </li> </b>
            HOG encodes the distribution of directions
            of gradients (oriented gradients) as features. These features
            capture the local shape within an image which is a crucial
            information for SBIR as the query images are essentially texture less
            sketches that describe the shape of objects.
            <br>
            The implementation of descriptor is as follows. 
            Gradients- magnitude and direction of the image are calculated. The image is 
            divided into windows of size 64 x 64. [The choice of this window size is described 
            in the experiments and results section below]. For each window, histogram of gradient 
            directions is calculated. The histogram is divided into 9 bins. Each pixel contributes the 
            weight(gradient magnitude) proportionally to the bins it's angle is falling between.
            To make the descriptor invariant to changes in illumination, the 64 x 64 window is 
            normalized by considering a block of size 128 x 128 which has 4 64 x 64 windows and 
            these 4 histograms (1 of each window) of size 9 x 1 are concatenated to a histogram of size 36 x 1. 
            And, this 128 x 128 block is slided along horizontally by 64 pixels and veritcally by 64 pixels 
            making it to a total of 3 x 3 positions as the images of shoe dataset are of size 256 x 256. This 
            implies that the feature vector is of size 3*3*36 = 324. A 256 x 256 = 65,536 image is 
            now represented with a feature vector of size 324.          
            <br>
            Calculating histogram over a patch not only makes the representation
            compact, but also makes it robust to noise in gradients. Operating on
            local windows makes the descriptor invariant to geometric and
            photometric transformations.

            <b> <li> SIFT  </li> </b>
            <b> <li> Hu Moments </li> </b>
            Hu Moments generates descriptors based on the outer contours of an shape. It is invariant to translation, size and rotation.
            We consider this a global shape feature descriptor.
            <br> Hu Moments is based on the idea of image moments which is a certain particular weighted average (moment) of the image pixels' intensities, or a function of such moments, usually chosen to have some attractive property or interpretation. We chose Hu Moments because it is fit for describing the countour of objects. To use it, we first find the contour of the sketch/image and then binarize the image based on this contour. Simple properties of the image which are found via image moments include area (or total intensity), its centroid, and information about its orientation.

        </ol>
        We used OpenCV's implementation of above descriptors.
        <h4>Retrieval Phase</h4>
        This is the phase in which the query image is processed to retrieve
        similar images.
        <ul>
            <li><b>Feature extraction</b></li>
            Similar to the feature extraction step of the pre-processing phase,
            we extract features from the input sketch image and form a feature vector.
            <li><b>Similarity comparison</b></li>
            <ul>
                <li><b> L2 Norm</b>  - L2 Norm/Euclidean distance metric is used to compare the features extracted using SIFT and Hu Moments.</li>
                <li><b>  L1 Norm</b> - L1 Norm/Manhattan/Cityblock distance metric is used to compare the features extracted using HOG. </li>

            </ul>

                We chose the distance metrics as per [5] which says that Manhattan distance (L1 norm) may be
                preferable to Euclidean distance (L2 norm) for the case of high dimensional data. As described in
                the approach section above, HOG features' dimensions are significantly higher than that of SIFT and Hu Moments.
         <br>
            The top 'k' similar images from the above step are retrieved as
            output.
        </ul>
        <br>

        <h3 style="color: #3366CC">Experiments and Results</h3>

        <h4>Experimental Setup</h4>
            <li><b>Dataset Used: <a href="https://www.eecs.qmul.ac.uk/~qian/Project_cvpr16.html">The Shoe Dataset </a></b><br>
                This dataset is organized into two categories of photos and sketches - shoe and chair. Images and sketches are of different scales and orientations. We first start with a subset of this dataset which has 200 images of shoes and chairs each and 200 sketches of shoes and chairs each.
            <li><b>Input for preprocessing</b> - 200 images of shoes and 200 images of chairs</li>
            <li><b>Output of preprocessing</b> - 400 feature vectors for each image</li>
            <li><b>Input for Retrieval phase</b> - 200 sketches of shoes and 200 sketches of chairs as query sketches</li>
            <li><b>Output of Retrieval phase</b> - Top k similar images for each query sketch</li>

        <h4>Evaluation metrics</h4>
                We will be evaluating the performance of our system (different descriptors and different parameters) using the
                below metrics.

                <ul>
                    <li><b>Precision at k:</b> For a given query sketch image,
                        precision at k images (P@k) is (e.g., P@10 or
                        "Precision at 10") corresponds to the number of
                        relevant images among the top k images retrieved.
                    </li>
                    <li><b>Average Precision at k per category:</b> We evaluate
                        the performance of our system for each category of
                        images in the dataset using this metric. Calculate P@k
                        for each sketch image in the category and return the
                        average of P@k scores.
                    </li>
                    <li><b>Average Precision at k per dataset:</b>
                        Every sketch image in the dataset as query image and
                        return the average of P@k score for each query.
                    </li>
                    <li><b>Recall at k:</b> For a given query sketch image,
                        recall at k images (R@k) is (e.g., R@10 or "Recall at
                        10") corresponds to the fraction of the number of
                        relevant images retrieved from the total relevant
                        images.
                    </li>
                    <li><b>Average Recall at k per category:</b>
                        We evaluate the performance of our system for each
                        category of images in the dataset using this metric.
                        Calculate R@k for each sketch image in the category and
                        return the average of R@k scores.
                    </li>
                    <li><b>Average Recall at k per dataset:</b>
                        Every sketch image in the dataset as query image and
                        return the average of R@k score for each query.
                    </li>
                </ul>
        <br>
        <h4>Baseline</h4>
            Since we have a 50/50 divide in the candidate images of the two catogories, the baseline of our approach will be 0.5.

        <br>

        <h4>Comparing the three desriptors by average precision-recall plots</h4>
            Here, by average we mean that we calculate the precision and recall values for
            each and all of the input sketches and then calculate the average performance.
            We provide the graphs both for over all and for each sketch input category.
            <ul>
                    <li><b>SIFT </b><br>
                        <img style="width: 33%;" alt="" src="sift_precision_recall_curve.png">
                        <img style="width: 33%;" alt="" src="sift_precision_recall_curve_chairs.png">
                        <img style="width: 33%;" alt="" src="sift_precision_recall_curve_shoes.png">
                    </li>
                    <li><b>HOG</b><br>
                        <img style="width: 33%;" alt="" src="hog64_precision_recall.png">
                        <img style="width: 33%;" alt="" src="hog64_precision_recall_chairs.png">
                        <img style="width: 33%;" alt="" src="hog64_precision_recall_shoes.png">
                    </li>
                    <li><b>Hu Moments</b><br>
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hu2.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_chairs_hu2.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_shoes_hu2.png">
                    </li>
            </ul>
            Analysis:
        <br>


        <h4>Selecting Parameters</h4>
            Here, we show the precision and recall values for different window sizes for HOG.
            <ul>
                    <li> <b> Window size = 16 x 16 </b> <br>
                        <img style="width: 33%;" alt="" src="hog16_precision_recall_curve.png">
                        <img style="width: 33%;" alt="" src="hog16_precision_recall_curve_chairs.png">
                        <img style="width: 33%;" alt="" src="hog16_precision_recall_curve_shoes.png">
                    </li>
                    <li> <b> Window size = 32 x 32 </b> <br>
                        <img style="width: 33%;" alt="" src="hog32_precision_recall_curve.png">
                        <img style="width: 33%;" alt="" src="hog32_precision_recall_curve_chairs.png">
                        <img style="width: 33%;" alt="" src="hog32_precision_recall_curve_shoes.png">
                    </li>
                    <li> <b> Window size = 64 x 64 </b> <br>
                        <img style="width: 33%;" alt="" src="hog64_precision_recall.png">
                        <img style="width: 33%;" alt="" src="hog64_precision_recall_chairs.png">
                        <img style="width: 33%;" alt="" src="hog64_precision_recall_shoes.png">
                    </li>
                    <li> <b>Window size = 128 x 128</b> <br>
                        <img style="width: 33%;" alt="" src="hog128_precision_recall.png">
                        <img style="width: 33%;" alt="" src="hog128_precision_recall_chairs.png">
                        <img style="width: 33%;" alt="" src="hog128_precision_recall_shoes.png">
                    </li>
            </ul>

        <br>

        <br>

        <h3 style="color: #3366CC">Qualitative results</h3>
        Success Cases
        <br>
        Failures



        <h3 style="color: #3366CC">Conclusion and future work</h3>
        We will be evaluating the performance of our current implementation of framework
        and feature extractors (HOG, SIFT and Hu Moments)
        with <a href="http://sketchy.eye.gatech.edu/explore/banana.html"> The Sketchy Database </a>
        which is the first large-scale collection of sketch-photo pairs. The
        dataset has 125 categories of objects with 100 images
        per category and 75,471 sketches of 12,500 objects. Unlike the shoe dataset,
        this database is benchmarked for fine-grained sketch-based image retrieval
        that is used to embed images and sketches of different scales and orientations
        in the same feature space using convolutional networks. Since the highly performing
        feature descriptor HOG is not rotation-scale invariant, we will try cascading the
        weak but rotation-scale invariant descriptors- SIFT, Hu Moments with HOG to improve
        performance with The Sketchy Database. Along with this, we may try options like
        query expansion discussed in the class, training SVM/CNN models with some/all of these features

        </li>

        <br><br>

        <h3 style="color: #3366CC">References/Citations</h3>
        [1] Li, Y. & Li, W. Machine Vision and Applications (2018) 29:
        1083.
        https://doi.org/10.1007/s00138-018-0953-8
        <br>[2] M. Eitz, K. Hildebrand, T. Boubekeur and M. Alexa,
        "Sketch-Based Image Retrieval: Benchmark and Bag-of-Features
        Descriptors," in IEEE Transactions on Visualization and Computer
        Graphics, vol. 17, no. 11, pp. 1624-1636, Nov. 2011.
        <br>[3] Xiao, Changcheng & Wang, Changhu & Zhang, Liqing & Zhang,
        Lei.
        (2015). Sketch-based Image Retrieval via Shape Words. 571-574.
        10.1145/2671188.2749360.
        <br>[4] C. Xiao, C. Wang, L. Zhang, and L. Zhang, “IdeaPanel,” in
        Proceedings of the 5th ACM on International Conference on
        Multimedia
        Retrieval-ICMR '15, pp. 667-668 (2015).
        <br>[5] Aggarwal C.C., Hinneburg A., Keim D.A. (2001) On the
        Surprising Behavior of Distance Metrics in High Dimensional Space.
        In: Van den Bussche J., Vianu V. (eds) Database Theory — ICDT 2001. ICDT 2001.
        Lecture Notes in Computer Science, vol 1973. Springer, Berlin, Heidelberg
        <br>[6] Ming-Kuei Hu, "Visual pattern recognition by moment invariants," in IRE Transactions on Information Theory, vol. 8, no. 2, pp. 179-187, February 1962.
        <br><br>

        <hr>
        <footer>
            <p>© Ramya Sree Boppana, Ang Deng, and Sanjana Garg </p>
        </footer>
    </div>
</div>

<br><br>

</body>
</html>

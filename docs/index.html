<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Computer Vision Class Project
        | CS, Georgia Tech | Fall 2019: CS 6476</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="css/bootstrap.css" rel="stylesheet">
    <style>
        body {
            padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
        }

        .vis {
            color: #3366CC;
        }

        .data {
            color: #FF9900;
        }
    </style>

    <link href="css/bootstrap-responsive.min.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
</head>

<body>
<div class="container">
    <div class="page-header">

        <!-- Title and Name -->
        <h1>Sketch-Based Image Retrieval</h1>
        <span style="font-size: 20px; line-height: 1.5em;"><strong>Ramya Sree Boppana (903456349), Ang Deng (902989694), and Sanjana Garg (903475801)</strong></span><br>
        <span style="font-size: 18px; line-height: 1.5em;">Fall 2019: CS 6476 Computer Vision: Class Project</span><br>Georgia
        Tech</span>
        <hr>

        <h4> <a href="proposal.html"> Proposal </a></h4>
        <!-- Introduction -->
        <h3 style="color: #3366CC">Abstract</h3>



        Sketch based image retrieval systems have many applications in daily life like
        digital library, search engines, crime prevention,
        photo sharing sites, and sensing remote systems.
        The traditional text-based image retrieval systems are limited
        as with growing database of images it is difficult and unrealistic to
        annotate all the images for text-based search.
        <br>
        In order to tackle this problem, we build a pipeline combining preprocessing the images (gray scaling and edge extraction) and extracting various feature descriptors (SIFT, HOG, and Hu Moments)
        from the images. Then, we take the extracted features from an input sketch and fetch the most similar k images from the database based on distance metrics corresponding to each type of feature descriptors.
        <br>
        Based on the test dataset which contains images from two different categories split in 50/50 way,
        we were able to achieve above baseline (50%) accuracy from all three descriptors that we implemented. Our next step will be to try to tweak the parameters of these feature
        extractors and to try out strategies to combine the features in order to improve the
        overall performance.



        <h3 style="color: #3366CC">Teaser Figure</h3>
        Sample output from our application:<br>

        &emsp;&emsp;&emsp;&emsp;&emsp;
        <img style="height: 120px;" alt="" src="chair16.png">
        <img style="height: 300px;" alt="" src="hog_chair16.png">


        <br><br>

        <h3 style="color: #3366CC">Introduction</h3>

        The traditional text-based image retrieval systems are limited as with growing database of images it is difficult and unrealistic to annotate all the images for text-based search. Using sketch to retrieve images also alleviates the need for generating accurate captions for existing images, which becomes a complex natural language processing problem as the description is required to be more specific and detailed. Sketches have a much greater potential to describe the content and exact details of the image than plain text and are a more direct way of expressing human thoughts than text abstraction.<br>
        Sketch based image retrieval systems have many applications in daily life like medical diagnosis, digital library, search engines, crime prevention, photo sharing sites, geographical information, and sensing remote systems. With the increasing use of touch screen devices, Sketch-Based Image Retrieval (SBIR) has relevant applications in e-commerce platforms also. <br>
        Currently we are working  with regular RBG photographs, but the skeches are only black and white.
        We are not designing any new way to solve this problem, but building from scratch using knowledge we obtained from class lectures and researches.<br>


        <!-- Approach -->
        <h3 style="color: #3366CC">Approach</h3>
        In our approach, we aim to tackle the following aspects of sketch-image
        differences:
        <ul>
            <li><b>Visual Cue Imbalance:</b> The sketches have a holistic shape
                and salient local shapes while the images are abundant in
                details on shape, texture, and color.
            </li>
            <li><b>Abstraction Gap:</b> The sketches are usually simplified
                version (missing details) of images with random distortion (the
                randomness in strokes) and unrealistic disproportion (object
                parts being unrealistically smaller or bigger).
            </li>
        </ul>

        <div style="text-align: center;">
            <img style="height: 250px;" alt=""
                 src="proposal_approach_flow_diagram.png">
        </div>
        The whole SBIR framework can be divided into two phases- pre-processing
        and retrieval. The framework is shown in the above figure.
        <h4>Pre-Processing Phase</h4>
        In this phase, we pre-process the dataset to extract features. For
        every image in the dataset, we perform the following steps.
        <h5>Feature/Edge extraction</h5>
        Convert the image to its gray intensity representation. Extract local edge or global shape features
        and encode them in a feature vector using representations like histogram.
        The histogram based features are efficient for similarity comparison
        and they can also tolerate random distortions in the sketches
        due to the grid division scheme in feature extraction.
        Edge/shape extraction addresses the visual cue imbalance. Since the sketches
        are generally composed of strokes which are mostly edges, it is
        intuitive to compare edge maps of images with sketches. This also addresses
        the simplification sub-aspect of the abstraction gap.
        <br>
        Some of the feature extraction methods we tried are described below. <br>
        <ol>
            <b> <li>  HOG - Histogram of Oriented Gradients  </li> </b>
            HOG encodes the distribution of directions
            of gradients (oriented gradients) as features.
            The implementation of descriptor is as follows.
            Gradients- magnitude and direction of the image are calculated. The image is
            divided into windows of size 64 x 64. [The choice of this window size is described
            in the experiments and results section below]. For each window, histogram of gradient
            directions is calculated. The histogram is divided into 9 bins. Each pixel contributes the
            weight(gradient magnitude) proportionally to the bins it's angle is falling between.
            To make the descriptor invariant to changes in illumination, the 64 x 64 window is
            normalized by considering a block of size 128 x 128 which has 4 64 x 64 windows and
            these 4 histograms (1 of each window) of size 9 x 1 are concatenated to a histogram of size 36 x 1.
            And, this 128 x 128 block is slided along horizontally by 64 pixels and veritcally by 64 pixels
            making it to a total of 3 x 3 positions as the images of shoe dataset are of size 256 x 256. This
            implies that the feature vector is of size 3*3*36 = 324. A 256 x 256 = 65,536 image is
            now represented with a feature vector of size 324.
            <br>
            Calculating histogram over a patch not only makes the representation
            compact, but also makes it robust to noise in gradients. Operating on
            local windows makes the descriptor invariant to geometric and
            photometric transformations.

            <b> <li> SIFT  </li> </b>
            The scale-invariant feature transform (SIFT) is used to detect local features in images. It transforms an image into a large collection of feature vectors, each of which is invariant to image translation, scaling, and rotation, partially invariant to illumination changes and robust to local geometric distortion. These features share similar properties with neurons in primary visual cortex that are encoding basic forms, color and movement for object detection in primate vision. Key locations are defined as maxima and minima of the result of difference of Gaussians function applied in scale space to a series of smoothed and resampled images. Low-contrast candidate points and edge response points along an edge are discarded. Dominant orientations are assigned to localized keypoints. These steps ensure that the keypoints are more stable for matching and recognition. SIFT descriptors robust to local affine distortion are then obtained by considering pixels around a radius of the key location, blurring and resampling of local image orientation planes.
            <b> <li> Hu Moments </li> </b>
            Hu Moments generates descriptors based on the outer contours of an shape. It is invariant to translation, size and rotation.
            We consider this a global shape feature descriptor.
            <br> Hu Moments is based on the idea of image moments which is a certain particular weighted average (moment) of the image pixels' intensities, or a function of such moments, usually chosen to have some attractive property or interpretation. We chose Hu Moments because it is fit for describing the countour of objects. To use it, we first find the contour of the sketch/image and then binarize the image based on this contour. One challenge with that is that a lot of the sketches do not have a closed contour. To counter that, we used the dilation + erosion operation to connect the broken lines. Simple properties of the image which are found via image moments include area (or total intensity), its centroid, and information about its orientation. Length of the descripter vector for Hu Moments is 7.


        </ol>
        We used OpenCV's implementation of above descriptors.
        <h4>Retrieval Phase</h4>
        This is the phase in which the query image is processed to retrieve
        similar images.
        <ul>
            <li><b>Feature extraction</b></li>
            Similar to the feature extraction step of the pre-processing phase,
            we extract features from the input sketch image and form a feature vector.
            <li><b>Similarity comparison</b></li>
            <ul>
                <li><b> L2 Norm</b>  - L2 Norm/Euclidean distance metric is used to compare the features extracted using SIFT and Hu Moments.</li>
                <li><b>  L1 Norm</b> - L1 Norm/Manhattan/Cityblock distance metric is used to compare the features extracted using HOG. </li>

            </ul>

                We chose the distance metrics as per [5] which says that Manhattan distance (L1 norm) may be
                preferable to Euclidean distance (L2 norm) for the case of high dimensional data. As described in
                the approach section above, HOG features' dimensions are significantly higher than that of SIFT and Hu Moments.
         <br>
            The top 'k' similar images from the above step are retrieved as
            output.
        </ul>


        <h3 style="color: #3366CC">Experiments and Results</h3>

        <h4>Experimental Setup</h4>
            <li><b>Dataset Used: <a href="https://www.eecs.qmul.ac.uk/~qian/Project_cvpr16.html">The Shoe Dataset </a></b><br>
                This dataset is organized into two categories of photos and sketches - shoe and chair. Images and sketches are of different scales and orientations. We first start with a subset of this dataset which has 200 images of shoes and chairs each and 200 sketches of shoes and chairs each.
            <li><b>Input for preprocessing</b> - 200 images of shoes and 200 images of chairs</li>
            <li><b>Output of preprocessing</b> - 400 feature vectors for each image</li>
            <li><b>Input for Retrieval phase</b> - 200 sketches of shoes and 200 sketches of chairs as query sketches</li>
            <li><b>Output of Retrieval phase</b> - Top k similar images for each query sketch</li>

        <h4>Evaluation metrics</h4>
                We will be evaluating the performance of our system (different descriptors and different parameters) using the
                below metrics.

                <ul>
                    <li><b>Precision at k:</b> For a given query sketch image,
                        precision at k images (P@k) is (e.g., P@10 or
                        "Precision at 10") corresponds to the number of
                        relevant images among the top k images retrieved.
                    </li>
                    <li><b>Average Precision at k per category:</b> We evaluate
                        the performance of our system for each category of
                        images in the dataset using this metric. Calculate P@k
                        for each sketch image in the category and return the
                        average of P@k scores.
                    </li>
                    <li><b>Average Precision at k per dataset:</b>
                        Every sketch image in the dataset as query image and
                        return the average of P@k score for each query.
                    </li>
                    <li><b>Recall at k:</b> For a given query sketch image,
                        recall at k images (R@k) is (e.g., R@10 or "Recall at
                        10") corresponds to the fraction of the number of
                        relevant images retrieved from the total relevant
                        images.
                    </li>
                    <li><b>Average Recall at k per category:</b>
                        We evaluate the performance of our system for each
                        category of images in the dataset using this metric.
                        Calculate R@k for each sketch image in the category and
                        return the average of R@k scores.
                    </li>
                    <li><b>Average Recall at k per dataset:</b>
                        Every sketch image in the dataset as query image and
                        return the average of R@k score for each query.
                    </li>
                </ul>
        <br>
        <h4>Baseline</h4>
            Since we have a 50/50 divide in the candidate images of the two catogories, the baseline of our approach will be 0.5.

        <br>

        <h4>Comparing the three descriptors by average precision-recall plots</h4>
            Here, by average we mean that we calculate the precision and recall values for
            each and all of the input sketches and then calculate the average performance.
            We provide the graphs both for over all and for each sketch input category.
            <ul>
                    <li><b>HOG</b><br>
                    Window size: 64 x 64 <br>
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog_chairs.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog_shoes.png">
                    </li>
                    <li><b>SIFT </b><br>
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_sift.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_sift_chairs.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_sift_shoes.png">
                    </li>
                    <li><b>Hu Moments</b><br>
                        <img style="width: 33%;" alt="" src=" precision_recall_plot_overall_hu.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hu_chairs.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hu_shoes.png">
                    </li>
            </ul>
            <h4> Analysis </h4>
            The above results are calculated for k values of 5,10,20,30,90,100,110,170,180,190 and 195.
            For k = 10, the average precision of HOG is 0.928, SIFT is 0.541 and Hu Moments is 0.6.
            For the category of chairs, average precision at k=10 for HOG is 0.914, SIFT is 0.858 and
            Hu Moments is 0.63. For the category of shoes, average precision at k=10 for HOG is 0.942,
            SIFT is 0.225 and Hu Moments is 0.572. For k = 190, the average recall of HOG is 0.605, SIFT
            is 0.49 and Hu Moments is 0.484. Overall, HOG performs better in terms of both precision and
            recall for both the image categories as it captures the local shape within an image which is
            a crucial information for SBIR as the query images are essentially texture less sketches
            that describe the shape of objects. Due to the absence of significant keypoints in
            the shoe sketches, performance of SIFT is poor for that category. Hu Moments is more primitive
            and is not biased towards any of categories as it captures global shape information. Compared to
            the other two descriptors, Hu Moments performs better in terms of memory and retrieval time
            because of the very short feature vector of length 7.
        <br>

        <h4>Parameter Selection for HoG</h4>
            Here, we show the precision and recall values for different window sizes for HOG.
            <ul>
                    <li> <b> Window size = 16 x 16 </b> <br>
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog16.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog16_chairs.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog16_shoes.png">
                    </li>
                    <li> <b> Window size = 32 x 32 </b> <br>
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog32.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog32_chairs.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog32_shoes.png">
                    </li>
                    <li> <b>Window size = 128 x 128</b> <br>
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog128.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog128_chairs.png">
                        <img style="width: 33%;" alt="" src="precision_recall_plot_overall_hog128_shoes.png">
                    </li>
            </ul>
            The window size is an important design choice to be made in the HOG descriptor.
            The default size is 8 x 8. The average precision at k=10 for 
            chairs at window size 16 x 16 is 0.407, window size 32 x 32 is 0.765, 
            64 x 64 is 0.914, 128 x 128 is 0.853. The average precision at k=10 for 
            shoes at window size 16 x 16 is 0.991, window size 32 x 32 is 0.985, 
            64 x 64 is 0.942, 128 x 128 is 0.872. As the window size increases, the performance 
            of chairs increases in terms of precision whereas the performance of shoes decreases.
            Window of 16 x 16 is too small for a chair image to capture any essential information whereas 
            its an appropriate size for shoes. Window of 128 x 128 is too big for 256 x 256 image and so it 
            does not perform well for both categories. We chose the size of 64 x 64
            which performs better for both categories on an average.
        <br>

        <br>

        <h3 style="color: #3366CC">Qualitative results</h3>
        <h4>Success Cases:</h4>
        <ul>
            <li> <b>HOG</b> <br>
                &emsp;&emsp;&emsp;&emsp;&emsp;
                <img style="height: 120px;" alt="" src="shoe176.png">
                <img style="height: 300px;" alt="" src="hog_shoe176.png">
            </li>
            <li> <b>SIFT</b> <br>
                &emsp;&emsp;&emsp;&emsp;&emsp;
                <img style="height: 120px;" alt="" src="chair16.png">
                <img style="height: 300px;" alt="" src="sift_chair16.png">
            </li>
            <li> <b>Hu Moments</b> <br>
                &emsp;&emsp;&emsp;&emsp;&emsp;
                <img style="height: 120px;" alt="" src="chair97.png">
                <img style="height: 300px;" alt="" src="hu_chair97.png">
            </li>
        </ul>
        <br>
        <h4>Failure Cases:</h4>
        <ul>
            <li> <b>HOG</b> <br>
                &emsp;&emsp;&emsp;&emsp;&emsp;
                <img style="height: 120px;" alt="" src="chair97.png">
                <img style="height: 300px;" alt="" src="hog_chair97.png">
            </li>
            <li> <b>SIFT</b> <br>
                &emsp;&emsp;&emsp;&emsp;&emsp;
                <img style="height: 120px;" alt="" src="shoe176.png">
                <img style="height: 300px;" alt="" src="sift_shoe176.png">
            </li>
            <li> <b>Hu Moments</b> <br>
                &emsp;&emsp;&emsp;&emsp;&emsp;
                <img style="height: 120px;" alt="" src="shoe187.png">
                <img style="height: 300px;" alt="" src="hu_shoe187.png">
            </li>
        </ul>
        <br>



        <h3 style="color: #3366CC">Conclusion and future work</h3>
        We will be evaluating the performance of our current implementation of framework
        and feature extractors (HOG, SIFT and Hu Moments)
        with <a href="http://sketchy.eye.gatech.edu/explore/banana.html"> The Sketchy Database </a>
        which is the first large-scale collection of sketch-photo pairs. The
        dataset has 125 categories of objects with 100 images
        per category and 75,471 sketches of 12,500 objects. Unlike the shoe dataset,
        this database is benchmarked for fine-grained sketch-based image retrieval
        that is used to embed images and sketches of different scales and orientations
        in the same feature space using convolutional networks. Since the highly performing
        feature descriptor HOG is not rotation-scale invariant, we will try cascading the
        weak but rotation-scale invariant descriptors- SIFT, Hu Moments with HOG to improve
        performance with The Sketchy Database. Along with this, we may try options like
        query expansion discussed in the class, training SVM/CNN models with some/all of these features

        </li>

        <br><br>

        <h3 style="color: #3366CC">References/Citations</h3>
        [1] Li, Y. & Li, W. Machine Vision and Applications (2018) 29:
        1083.
        https://doi.org/10.1007/s00138-018-0953-8
        <br>[2] M. Eitz, K. Hildebrand, T. Boubekeur and M. Alexa,
        "Sketch-Based Image Retrieval: Benchmark and Bag-of-Features
        Descriptors," in IEEE Transactions on Visualization and Computer
        Graphics, vol. 17, no. 11, pp. 1624-1636, Nov. 2011.
        <br>[3] Xiao, Changcheng & Wang, Changhu & Zhang, Liqing & Zhang,
        Lei.
        (2015). Sketch-based Image Retrieval via Shape Words. 571-574.
        10.1145/2671188.2749360.
        <br>[4] C. Xiao, C. Wang, L. Zhang, and L. Zhang, “IdeaPanel,” in
        Proceedings of the 5th ACM on International Conference on
        Multimedia
        Retrieval-ICMR '15, pp. 667-668 (2015).
        <br>[5] Aggarwal C.C., Hinneburg A., Keim D.A. (2001) On the
        Surprising Behavior of Distance Metrics in High Dimensional Space.
        In: Van den Bussche J., Vianu V. (eds) Database Theory — ICDT 2001. ICDT 2001.
        Lecture Notes in Computer Science, vol 1973. Springer, Berlin, Heidelberg
        <br>[6] Ming-Kuei Hu, "Visual pattern recognition by moment invariants," in IRE Transactions on Information Theory, vol. 8, no. 2, pp. 179-187, February 1962.
        <br><br>

        <hr>
        <footer>
            <p>© Ramya Sree Boppana, Ang Deng, and Sanjana Garg </p>
        </footer>
    </div>
</div>

<br><br>

</body>
</html>

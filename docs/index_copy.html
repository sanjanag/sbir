<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <title>Computer Vision Class Project
        | CS, Georgia Tech | Fall 2019: CS 6476</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="css/bootstrap.css" rel="stylesheet">
    <style>
        body {
            padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
        }

        .vis {
            color: #3366CC;
        }

        .data {
            color: #FF9900;
        }
    </style>

    <link href="css/bootstrap-responsive.min.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
</head>

<body>
<div class="container">
    <div class="page-header">

        <!-- Title and Name -->
        <h1>Sketch-Based Image Retrieval</h1>
        <span style="font-size: 20px; line-height: 1.5em;"><strong>Ramya Sree Boppana (903456349), Ang Deng (902989694), and Sanjana Garg (903475801)</strong></span><br>
        <span style="font-size: 18px; line-height: 1.5em;">Fall 2019: CS 6476 Computer Vision: Class Project</span><br>Georgia
        Tech</span>
        <hr>

        <h4> <a href="proposal.html"> Proposal </a></h4>
        <!-- Introduction -->
        <h3>Abstract</h3>

 

        Sketch based image retrieval systems have many applications in daily life like
        Medical diagnosis, digital library, search engines, crime prevention,
        photo sharing sites, geographical information, and sensing remote
        systems. The traditional text-based image retrieval systems are limited
        as with growing database of images it is difficult and unrealistic to
        annotate all the images for text-based search.
        <br>
        In order to tackle this problem, we build a pipeline combining preprocessing 
        the images (gray scaling edge extraction) and extracting various feature descriptors
        (SIFT, HOG, and Hu Moments)
        from the images. Then, we take the extracted features from a input sketch and fetch the most similar k images from the database based on distance metrics corresponding to each type of feature descriptors.
        <br>
        Based on the test dataset which contains images from two different categories split in 50/50 way, 
        we were able to achieve above baseline (50%) accuracy from all three descriptors that we implemented. Our next step will be to try to tweak the parameters of these feature
        extractors and to try out strategies to combine the features in order to improve the 
        overall performance.



        <h3>Teaser Figure</h3>
        Sample output from our application:<br>

        &emsp;&emsp;&emsp;&emsp;&emsp;
        <img style="height: 120px;" alt="" src="chair16.png">
        <img style="height: 300px;" alt="" src="hog_chair16.png">


        <br><br>

        <h3>Introduction</h3>

        The traditional text-based image retrieval systems are limited as with growing database of images it is difficult and unrealistic to annotate all the images for text-based search. Using sketch to retrieve images also alleviates the need for generating accurate captions for existing images, which becomes a complex natural language processing problem as the description is required to be more specific and detailed. Sketches have a much greater potential to describe the content and exact details of the image than plain text and are a more direct way of expressing human thoughts than text abstraction.<br>
        Sketch based image retrieval systems have many applications in daily life like Medical diagnosis, digital library, search engines, crime prevention, photo sharing sites, geographical information, and sensing remote systems. With the increasing use of touch screen devices, Sketch-Based Image Retrieval (SBIR) has relevant applications in e-commerce platforms also. <br>
        Currently we are with regular RBG photographs, but the skeches are only black and white.
        We are not designming any new way to solve this problem, but building from scratch using knowledge we obtained from class lectures and reseaches.<br>


        <!-- Approach -->
        <h3>Approach</h3>
        <!-- In our approach, we aim to tackle the following aspects of sketch-image differences: <br>
        <h4>Visual Cue Imbalance</h4>
        The sketches usually have a holistic shape and salient local shapes while the images are abundant in details on shape, texture, and color.
        <h4>Abstraction Gap</h4>
        The sketches are usually a simplified version (missing details) of images with random distortion (the randomness in strokes) and unrealistic disproportion (object parts being unrealistically smaller or bigger).
        <br>
         -->

        <!-- Approach Figure -->
        In our approach, we aim to tackle the following aspects of sketch-image
        differences:
        <ul>
            <li><b>Visual Cue Imbalance:</b> The sketches have a holistic shape
                and salient local shapes while the images are abundant in
                details on shape, texture, and color.
            </li>
            <li><b>Abstraction Gap:</b> The sketches are usually simplified
                version (missing details) of images with random distortion (the
                randomness in strokes) and unrealistic disproportion (object
                parts being unrealistically smaller or bigger).
            </li>
        </ul>

        <div style="text-align: center;">
            <img style="height: 250px;" alt=""
                 src="proposal_approach_flow_diagram.png">
        </div>
        The whole SBIR framework can be divided into two phases- pre-processing
        and retrieval. The framework is shown in the above figure.
        <h4>Pre-Processing Phase</h4>
        In this phase, we pre-process the dataset to extract features. For
        every image in the dataset, we perform the following steps.
        <ul>
            <li><b>Edge extraction</b></li>
            Convert the image to its gray intensity representation.
            <br>Extract local edge features using the Canny edge detector. Edge
            extraction addresses the visual cue imbalance. Since the sketches
            are generally composed of strokes which are mostly edges, it is
            intuitive to compare edge maps of images with sketches. We use a
            high threshold to extract only the salient edges and thus address
            the simplification sub-aspect of the abstraction gap.
            <li><b>Feature extraction</b></li>
            In this step, we encode the edge feature maps to representations
            that are efficient for similarity comparison.<br>
            We extract histogram-based local features like SIFT, Histogram of
            Oriented Gradients (HOG) and global features like shape. The
            histogram features can tolerate random distortions in the sketches
            due to the grid division scheme in feature extraction.
        </ul>
        <h4>Retrieval Phase</h4>
        This is the phase in which the query image is processed to retrieve
        similar images.
        <ul>
            <li><b>Feature extraction</b></li>
            Similar to the feature extraction step of the pre-processing phase,
            we extract a histogram of features from the input sketch image.
            Since sketches are close to the edge map of images, we do not have
            the step of edge extraction in this phase.
            <li><b>Similarity comparison</b></li>
            To compare the histogram-based features of the input sketch and the
            dataset images, we plan to use one or more of the metrics to
            calculate the similarity between histograms like city block,
            cosine, Chi-square, Euclidean distance, and histogram intersection
            distances. <br>
            The top 'k' similar images from the above step are retrieved as
            output.
        </ul>
        <br>

        

        <h3>Experiments and Results</h3>
        
        <h4>Experimental Setup</h4>
            <li><b>Dataset Used: </b><b>The Shoe Dataset</b><br>
                This dataset is organized into two categories of photos and sketches - shoe and chair and has a total of 1,432 sketch-photo pairs. Images and sketches are of the scale and orientation.<br> 
                <a href="https://www.eecs.qmul.ac.uk/~qian/Project_cvpr16.html">https://www.eecs.qmul.ac.uk/~qian/Project_cvpr16.html</a>
                <br>
                We first start with the two category dataset <b>Shoe</b> for
                    our problem, to test our features.</li>
            <li><b>Input</b><br>
                200 images of shoes and 200 images of chairs.

            <li><b>Output</b><br>
                For the input sketch, the best k matches from the total of 400 candidate images.
            </li>

        <h4>Evaluation metrics</h4>
                We will be evaluating the performance of our system (different descriptors and different parameters) using the
                below metrics.
                <ul>
                <li><b>Precision and Recall at k:</b> For a given query sketch
                    image,
                    precision at k images (P@k) is (e.g., P@10 or
                    "Precision at 10") corresponds to the number of
                    relevant images among the top k images retrieved.
                </li>
                <li>
                    <b>Average Precision at k per category:</b> We evaluate
                    the peformance of our system for each category of
                    images in the dataset using this metric. This is
                    calculated as follows for a category.
                    Consider each sketch image in the category as query
                    and
                    calculate P@k for that query image. Calculate mean
                    of
                    the P@k scores for each query.
                </li>
                
            </ul>
            
        
        </ol>
        <h4>Experiments</h4>
        Using the above setup (datasets and evaluation metrics) following
        are
        some of the experiments we plan to perform.
        <ul>
            <li>Experiment with different image features for different
                categories of objects and evaluate which features perform
                better for a category.
            </li>
                <ul>
                <li> For Hu Moments, based on the Shoes Dataset, the performance for both 
                    categories average to be about the same (between 0.6 to 0.7 accuracy). 
                </li>
                </ul>
            <li>Experiment with different datasets - Shoe and The Sketchy
                and
                evaluate how the system performs for different scales and
                orientations of sketches.
            </li>
        </ul>
        <h3>Qualitative results</h3>
        Show several visual examples of inputs/outputs of your system (success cases and failures) that help us better understand your approach.
        <h3>Conclusion and future work</h3>
        Conclusion would likely make the same points as the abstract. Discuss any future ideas you have to make your approach better.


        <br><br>

        <h3>References/Citations</h3>
        [1] Li, Y. & Li, W. Machine Vision and Applications (2018) 29:
        1083.
        https://doi.org/10.1007/s00138-018-0953-8
        <br>[2] M. Eitz, K. Hildebrand, T. Boubekeur and M. Alexa,
        "Sketch-Based Image Retrieval: Benchmark and Bag-of-Features
        Descriptors," in IEEE Transactions on Visualization and Computer
        Graphics, vol. 17, no. 11, pp. 1624-1636, Nov. 2011.
        <br>[3] Xiao, Changcheng & Wang, Changhu & Zhang, Liqing & Zhang,
        Lei.
        (2015). Sketch-based Image Retrieval via Shape Words. 571-574.
        10.1145/2671188.2749360.
        <br>[4] C. Xiao, C. Wang, L. Zhang, and L. Zhang, “IdeaPanel,” in
        Proceedings of the 5th ACM on International Conference on
        Multimedia
        Retrieval-ICMR '15, pp. 667-668 (2015).

        <br><br>

        <hr>
        <footer>
            <p>© Ramya Sree Boppana, Ang Deng, and Sanjana Garg </p>
        </footer>
    </div>
</div>

<br><br>

</body>
</html>
